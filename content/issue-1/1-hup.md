互联网从高速增长的莽荒纪元，逐渐过度到更智能的信息化时代。再加上人口红利的逐渐消退，人力成本也越来越高。很多公司在持续输出商业化产品的同时，对内降本增效的呼声也越来越强烈。尤其是创业十年以上的公司，早年的异构体系对研发的心智考验也越发的大。大势之下，好大夫在线技术中心也在积极应对降本增效带来的挑战，经过一年的迭代，目前已给研发团队交付了自运营平台。接下来我给大家分享一下其中的酸甜苦辣。

聊一下To-D的解决方案。

## 莽荒纪，生生不息

## 群雄割据，PaaS林立

## 顺应民意，合而为一
<!-- ## 离谱，居然差一个网址导航？ -->
以下场景相信大家都不陌生。咱们知道，不会偷懒的程序员，不是好的程序员，为了提效，各自发挥十八般武艺，自研小工具，所谓的PaaS层出不穷。
- 测试用例每次需要准备基境数据，那咱们就整个mock数据的小工具吧。
- 支付渠道越来越多，配置完小半天就没了，那咱们整个一键配置支付环境的小工具吧。
- 再有个小工具，能自由控制合作方域名回调测试，也是极好的。
- 如果咱们新sql从开发环境到产线环境，每一步都被验证过，将能提升稳定性，嗯，那就整个SQL PaaS平台吧。
- 对了对了，咱们经常处理消息的分发，再整个消息中间件的PaaS就完满了。
- 还不够，帮我们把项目从初始化，到最终交付产线整个流水线吧，最好能无人值守。
- ...

研发、测试、运维针对自己的日常工作，都在往自动化，智能化方向迭代。大家也享受着各种小工具带来的便利，同时又忍受着小工具带来的痛。
- 小工具的同质化，咱们知道组织架构，存在康威定律，为了减少沟通成本，往往演化出自己的一套工具链。重复的基建，也内耗了不少研发成本。
- 新员工的培训成本的增加，随着工具集的丰富，日常任务需要频繁地切换上下文，造成人力损耗。
- 随着工具集的丰富，往往在关键时刻找不到入口，更有甚者，低频的修复工具，在应急的时候，因年久失修而掉链子。

于是乎百宝箱的概念被提出来，咱们能不能把这些工具都归拢到一块，再按角色做好分类，再整合同质化的工具，这样打包放一块不就方便使用了吗?
继续之前的康威定律，前端弄了个构建平台，运维弄个了devops,测试弄了个自动化测试平台，基础架构弄了个治理平台。

到这里，提效只差一个网址导航了，再来一个首页，把各个平台的菜单展开，统一纳入进来就完美了。
[差个网址导航的图]

网址导航，是把选择权交给了用户，罗列一堆工具链的入口。
- 有流量抓取分析的URL集合；
- 有sonar代码扫描的入口；
- 有sentry异常诊断的入口；
- 有添加短信模板的入口；
- 有提sql上线的入口等等等等。
事实上这对研发的心智要求非常高。然而在精细化分开的当下，要想了解方方面面，几乎很难做到。随着功能的堆砌，导航网址越来越多。
有时候为了解决一个问题，需要从多个入口之间不停地跳来跳去，最后我们得到了一个集多功能于一身的缝合怪。

这与我们的初衷违背，我们要的不是瑞士军刀，只要一把能生存利器而已。

这种简单的整合，其实并没有解决我们面临的真正痛点。
- 跨平台交互之间的割裂感，尤其是页面交互体验
- 重复开发的基础模块，组织架构管理模块，用户中心，消息通知模块
- 紊乱的账号体系
- 紊乱的部署形态
- 紊乱的用户交互
- 缺乏系统性，都是想到哪做到哪

这些痛点，仿佛沉淀在时间长河里的泥沙，需要来一次彻底的清理才能奔赴向前。

## 破而后立，融而为一
既然要做，那就要做的彻底，也就需要深度整合各个已经存在的平台。由于康威定律的存在，可以想象颠覆性改革，注定需要面对巨大的阻力。
为了克服困难，需要从更大的目标上对齐，把蛋糕做大，做出来的产品令人向往，才能达成共识，实现持久共赢。

### 从订立目标开始

我们的目标，围绕产品交付的整个生命周期展开，打造辅助研发测试的智能工作台，并保障稳定性和提升效率。

需要将研发测试日常工作，事件化，可视化，并能持续追踪，其实是对工作流的一种可视化，让研发测试更加聚焦具体的工作项。

想象一下这个场景，A服务出现接口响应慢的告警，研发收到告警事件提醒后，打开我的工作台，他将看到到：
- 实时告警事件
- 历史流量同比环比
- 给出历史告警操作记录
- 给出最近该服务上线频次

研发就可以进行相应的常规操作，扩容，或限流，或调整告警阈值，抑或是回滚代码，当然部分操作还会发起工单审核。直接上图，大家能看的更直观。
[图、告警处理]

告警处理只是产品持续交付其中的一个小环节，可以看到实时工作台，整合了历史趋势，操作记录，上线频次，以及告警响应处理动作。
![](/static/s1/1/hup_004.png)

我们需要将这些离散的场景，抽成一个个响应事件，再串接起来，完成一项具体的工作。
后续随着数据的慢慢积累，和学习研发的操作行为，将这些经验固化到工作台中，更加智能地辅助研发做决策。


### 规划好清晰的路线

![](/static/s1/1/hup_006.png)
经过上面场景的分析，我们是围绕产品的持续交付展开的，涉及质量，安全，成本，效率等方方面面。咱们不可能一口吃个胖子，需要慢慢来，一份好的RoadMap，就像一盏指路明灯。
[图road_map]


### 快速落地基建，打响HUP第一枪

大饼画完了，就需要择个良辰吉日，动工了。2022-06-06 06:06PM，自运营工作台举行了开工仪式，投票命名HUP(haodf union platform)。为了提升信心，需要快捷完成一期基建。

这里先看一下整体拓扑。
![](/static/s1/1/hup_001.jpg)

基建，决定了平台的上限，是场景化迭代的保障。
所以一期的基建主要是通用服务的建设。接下来，我们一块了解下几个关键组件是如何协同工作的。
[图各个组件的位置关系]

**单点登录(SSO)**

之前各部门自研的PaaS平台，以及引入的第三方开源平台，大部分各自实现了一套用户管理模块。当有员工入职或者离职的时候，需要各个平台保障信息的同步，带来了额外的维护成本，以及安全风险问题，统一账号体系势在必行。
先看一下拓扑图。
![](/static/s1/1/authelia.jpg)

这期实现的单点登录，是基于Authelia共享顶级域的cookie模式。访问example.net任意二级域名，只要登录了一次就不用再次登录了。也就是说 a.example.net和b.example.net能实现单点登录，即*.example.net全域互通。
这些域名都需要经过sso认证之后才能放行，顺带也就接管了这些域名的DNS解析，规范了内部域名的使用。
Authelia用户对接LDAP，统一管理，人员变动，只用同步LDAP即可。
Authelia自带白名单功能，针对特殊的场景，可以配置免登录策略。
Authelia自带二次验证和设备认证，针对高危动作，如删除队列可配置二次验证策略。
由于我们服务都部署在k8s中，只用配置相应的ingress策略即可实现SSO。如果大家感兴趣。后续可以出一个单独的番外篇，详细介绍如何基于Authelia实现SSO。


**组织架构及权限控制**
[图，组织架构协作的] 首次尝试DDD，领域驱动设计，去实现。
权限认证的图
菜单图
引出整体的交互
有eureka 也有 基于k8s svc模式，接口层面基于服务发现。

[老系统交互的流程图https://www.processon.com/diagraming/6315603d1e0853187c124bb1]

页面采用嵌套的模式
组织架构，权限控制，菜单管理


**工单体系**

**老平台及开源项目平滑融合到工作台**
[图，多放一些平台相关的图形实例，可以采用九宫格模式] 或者电影图片，叠加的效果

迁移，基于vue的，前后端分离的全部迁移过来。


由于咱们不可能所有的内容都

咱们首先要解决的是老

低代码，header标识，判断来源

平滑切换，新域名，别名分组解析

### 孵化前沿创新，提升HUP影响力
[云端开发示意]

[CI/CD示意]

[尝试DDD,放领域实现的示意图]

[低代码平台]
云端开发，多版本，CI/CD流水线，go框架体系，领域模型DDD，事件总线，分布式事务
所有服务都部署在k8s，支持多集群部署和管理

### 重构高频场景，实现HUP价值
20%的功能，满足日常80%的场景。找到高频使用的场景去深入改造、小程序的发布。


小程序发布，原先在一台mac机器上，现在迁移到容器里，重新改造。
发布工程，一开始是一个缝合怪
![](/static/s1/1/hup_008.png)


### 打磨产品细节，提升HUP价值

魔鬼往往隐藏在细节里，细节决定了产品能走多远。从0到1往往只是盖版，从1到n才真正决定产品的价值。

在HUP迭代的过程中，我们也聚焦细节的质量，提升平台的价值。一起看一下这几个问题。

**如何在效率和稳定之间做平衡？**

![](/static/s1/1/hup_009.jpg)
场景化迭代，不只是实现某一项功能，不仅需要考虑服务于用户，还需要考虑服务于产品交付。有些场景化甚至是重新定义了研发测试部分使用习惯。

拿发布工程为例，服务上线必须经历开发环境，测试环境，最终才能到产线环境。更早的时候，开发是能跳过所有环节，直接发布到产线的，规范后的上线流程，虽然增加了上线时间，但能保障交付的产品被充分验证过。

但如果测试只想在测试环境测一项功能，就无需从开发环境再到测试环境，测试可以直接发布到测试环境，但这种发布不允许上线。

等等，发布工程做了大量的限定，从安全到效率寻找平衡点。当然这是漫长的过程，需要不断的调节，这也是目前吐槽最多的地方。


**能否提前设计好各个环节？**

HUP是好大夫服务的智能管家，管家着好大夫所有微服务的整个生命周期，从仓库代码到产线运行，以及服务治理。

这是一个庞大的体系，注定很难穷举所有场景，很难以上帝的视角提前设计好各个环节。

在前面打造产品交付流水线的时候就可以看出来，针对交付的各个环节，我们是按模块化设计的，支持动态可插拔。

跨服务交互也是基于事件发布订阅，整个生态是很多个服务组装在一起协同工作的。

我们将日常工作事件化，抽象成不同场景的工作流，配合工作流的钩子，动态调配日常工作，直到固化成一条工作经验。
[各种工作流]

**如何解决指导产品迭代？**

这里其实等价于，如何量化产出的问题。不仅需要量化HUP平台的价值，还需要量化研发测试的工作价值。

为了避免黑盒，我们借助于MDD思想(Metrics-Driven Development)，要求各个组件提炼出健康指标和反映内在价值的指标。
[指标上报流程图]



### 文化自信，传承HUP薪火

随着一期期的迭代，伴随着大家的吐槽，HUP快速成长，也承载着越来越多的希望。

作为平台的研发的我们，也逐渐自觉了新的身份认同--HUPer，沉淀了口号--Thinking In SRE.

HUPer做的不只是平台工具，而是在做一种企业文化，我们希望将HUP发展成一种文化符号，在不知不觉中更新大家的认知，吸纳更多的人共建生态。

我们知道产品交付，不只是发布到产线，而这恰恰只是开始，HUP作为研发测试工作台入口，势必承载了服务运营的职责，保障全站可用性和稳定性，即是职责，亦是使命。

HUPer们也在不断布道，组建线下沙龙，宣讲SRE的理念。

每一期的HUP版本迭代也会取一个代号，召开发布会，并准备周边礼品。

我们还会举办团建和周年庆，对有贡献的同学发放印有logo的文化衫，办公文具，书签等等。

慢慢地形成了一种文化传承，形成了开源的氛围，促进了HUP前行。


## 乘风破浪，征程再起

HUP目前重构了整个发布工程，完成了一阶段的交付，二阶段需要围绕交付后的服务运营展开，重新梳理SRE治理体系，提高服务的稳定性和可用性，缩短异常排查的时长。

另外一个方向，HUP目前大部分能力是基于私有云的解决方案，后续会补齐面向公有云和边缘计算的版块，为社区贡献一份自己的力量。

We are Keep running, Keep thinking.